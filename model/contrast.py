import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch import Tensor


class SupConLoss(nn.Module):

    def __init__(self, temperature=0.07, scale_by_temperature=True):
        super(SupConLoss, self).__init__()
        self.temperature = temperature
        self.scale_by_temperature = scale_by_temperature

    def forward(self, features, labels=None, mask=None, temperature=-1, soft=False):
        #feature size:[batch_size, hidden_dim]
        #labels size: [batch_size]
        #mask size: [batch_size, batch size], mask(i,j)=1 iff label(i)=label(j)
        #output: loss value

        if temperature > 0:
            self.temperature = temperature

        device = (torch.device('cuda') if features.is_cuda else torch.device('cpu'))
        features = F.normalize(features, p=2, dim=1)
        batch_size = features.shape[0]

        #not enough samples to compare
        if batch_size < 4:
            loss = torch.zeros(())
            return loss.to(device)

        #mask can be generated by label, and cannot be defined at the same time
        if labels is not None and mask is not None:
            raise ValueError('Cannot define both `labels` and `mask`')

        #adopt unsupervised learning by set mask to an identity matrix
        elif labels is None and mask is None:
            mask = torch.eye(batch_size, dtype=torch.float32).to(device)

        #generate mask via label
        elif labels is not None:
            labels = labels.contiguous().view(-1, 1)
            if labels.shape[0] != batch_size:
                raise ValueError('Num of labels does not match num of features')

            #mask(i,j)=1 iff label(i)=label(j)
            if not soft:
                mask = torch.eq(labels, labels.T).float().to(device)

            #mask(i,j) relative to |label(i)-label(j)|
            else:
                labels = labels.float()
                diff = F.pdist(labels, p=1).to(device)
                mask = torch.zeros([batch_size, batch_size]).float().to(device)
                mask[np.triu_indices(batch_size, k=1)] = diff
                new_mask = mask.clone()
                mask += new_mask.T

                min_val = mask.min()
                max_val = mask.max()
                if max_val != min_val:
                    mask = (mask - min_val) / (max_val - min_val)
                else:
                    if max_val != torch.tensor(0.):
                        mask = mask / max_val
                mask = 1.-mask
            #print(mask)

        #mask is not none, then label is not required
        else:
            mask = mask.float().to(device)

        #compute logits (use dot product to measure similarity)
        anchor_dot_contrast = torch.div(torch.matmul(features, features.T), self.temperature)

        #calculate the max value of each row and subtract it from the logits for numerical stability (optional)
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()
        #logits = anchor_dot_contrast

        exp_logits = torch.exp(logits)

        #construct logit mask by setting (i,i) to 0, since they represent the similarity between samples and themselves
        #logits_mask = torch.ones_like(mask) - torch.eye(batch_size).to(device)
        positives_mask = mask
        #positives_mask = mask * logits_mask
        negatives_mask = 1. - mask

        #number of positive samples except sample itself
        num_positives_per_row = torch.sum(positives_mask, axis=1)
        denominator = torch.sum( exp_logits * negatives_mask, axis=1, keepdims=True) + torch.sum(exp_logits * positives_mask, axis=1, keepdims=True)

        log_probs = logits - torch.log(denominator)
        if torch.any(torch.isnan(log_probs)):
            raise ValueError("Log_prob has nan!")

        #calculate log-likelihood for class with at least one positve sample pair
        log_probs = torch.sum(log_probs * positives_mask, axis=1)[num_positives_per_row > 0] / num_positives_per_row[num_positives_per_row > 0]

        loss = -log_probs
        loss = loss.mean()
        if self.scale_by_temperature:
            loss = loss*self.temperature
        if torch.isnan(loss):
            raise ValueError("Loss equal to nan!")
        return loss


#test loss function
if __name__ == '__main__':
    print("1")
    test_cl = SupConLoss()
    test_tensor = torch.zeros([4,8])
    test_label = torch.tensor([0,1,1,0])
    print(test_label.size())
    loss = test_cl(test_tensor, test_label, soft=True)
    loss = test_cl(test_tensor, test_label)
    pass